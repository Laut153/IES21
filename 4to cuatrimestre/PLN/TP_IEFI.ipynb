{"cells":[{"cell_type":"markdown","metadata":{},"source":["# IEFI Lautaro Santos Da Silveira"]},{"cell_type":"markdown","metadata":{},"source":["En el siguiente trabajo, se propone el desafío de identificar si una noticia es o no una Fake News. \n","Para tomar esa decisión primero se deberá aplicar herramientas de Procesamiento del Lenguaje Natural (NLP), y luego se procederá a a la eleccion del modelo que mejor aprenda. Se contará con modelos de SVM, RandomForest, Arbol de desicion y Regresion Logistica."]},{"cell_type":"markdown","metadata":{},"source":["### Librerias"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36911,"status":"ok","timestamp":1667176270935,"user":{"displayName":"Lautaro Santos Da Silveira","userId":"13309001856644337549"},"user_tz":180},"id":"rUEpifXpzpRP","outputId":"c4475f24-d917-406c-f33f-9151d986284b"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["Collecting es-core-news-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.4.0/es_core_news_sm-3.4.0-py3-none-any.whl (12.9 MB)\n","     ---------------------------------------- 12.9/12.9 MB 4.0 MB/s eta 0:00:00\n","Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from es-core-news-sm==3.4.0) (3.4.3)\n","Requirement already satisfied: setuptools in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (58.1.0)\n","Requirement already satisfied: jinja2 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.1.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.7)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.64.0)\n","Requirement already satisfied: pathy>=0.3.5 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: numpy>=1.15.0 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.23.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.10.2)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.5)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.4.5)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.0)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (21.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.9)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.28.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.4.0)\n","Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2022.9.24)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.26.12)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.0.3)\n","Requirement already satisfied: colorama in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.4.6)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\usuario\\desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.1.1)\n","✔ Download and installation successful\n","You can now load the package via spacy.load('es_core_news_sm')\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","nltk.download('punkt')\n","from nltk import regexp_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","!python -m spacy download es_core_news_sm\n","import spacy\n","nlp= spacy.load('es_core_news_sm')\n","import re\n","from collections  import Counter\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{},"source":["### Set de datos"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1667177005067,"user":{"displayName":"Lautaro Santos Da Silveira","userId":"13309001856644337549"},"user_tz":180},"id":"dWjx7ueK0Caw","outputId":"e3f3566a-ee02-465c-f8ec-273fa80968e8"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>class</th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>True</td>\n","      <td>Algunas de las voces extremistas más conocida...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>True</td>\n","      <td>Después de casi dos años y medio de luchas po...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>True</td>\n","      <td>Dos periodistas birmanos de la agencia Reuters...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>True</td>\n","      <td>El Cuerpo Nacional de Policía ha detenido a cu...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>True</td>\n","      <td>El desfile de la firma en Roma se convierte en...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1995</th>\n","      <td>True</td>\n","      <td>El Consejo de Gobierno ha dado su visto bueno...</td>\n","    </tr>\n","    <tr>\n","      <th>1996</th>\n","      <td>True</td>\n","      <td>Investigadores valencianos han desarrollado u...</td>\n","    </tr>\n","    <tr>\n","      <th>1997</th>\n","      <td>True</td>\n","      <td>Los arrestados actuaban en coches y en establ...</td>\n","    </tr>\n","    <tr>\n","      <th>1998</th>\n","      <td>True</td>\n","      <td>El Rey ha encargado este miércoles a Pedro Sá...</td>\n","    </tr>\n","    <tr>\n","      <th>1999</th>\n","      <td>True</td>\n","      <td>Las pruebas realizadas en el Centro Nacional ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2000 rows × 2 columns</p>\n","</div>"],"text/plain":["      class                                               Text\n","0      True   Algunas de las voces extremistas más conocida...\n","1      True   Después de casi dos años y medio de luchas po...\n","2      True  Dos periodistas birmanos de la agencia Reuters...\n","3      True  El Cuerpo Nacional de Policía ha detenido a cu...\n","4      True  El desfile de la firma en Roma se convierte en...\n","...     ...                                                ...\n","1995   True   El Consejo de Gobierno ha dado su visto bueno...\n","1996   True   Investigadores valencianos han desarrollado u...\n","1997   True   Los arrestados actuaban en coches y en establ...\n","1998   True   El Rey ha encargado este miércoles a Pedro Sá...\n","1999   True   Las pruebas realizadas en el Centro Nacional ...\n","\n","[2000 rows x 2 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["#df= pd.read_excel(r'D:\\Escritorio\\IES21\\4to cuatrimestre\\PLN\\Noticias Verdaderas y Falsas.xlsx')\n","df= pd.read_excel(r'C:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\IES21\\4to cuatrimestre\\PLN\\Noticias Verdaderas y Falsas.xlsx')\n","df"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1667177005070,"user":{"displayName":"Lautaro Santos Da Silveira","userId":"13309001856644337549"},"user_tz":180},"id":"yhhKfs3Q73HS","outputId":"15fb4c85-e4e3-46eb-d48e-f9d0c0101f0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 2000 entries, 0 to 1999\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   class   2000 non-null   bool  \n"," 1   Text    2000 non-null   object\n","dtypes: bool(1), object(1)\n","memory usage: 17.7+ KB\n"]}],"source":["df.info()"]},{"cell_type":"markdown","metadata":{},"source":["### Funcion de preprocesamiento"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1667177005073,"user":{"displayName":"Lautaro Santos Da Silveira","userId":"13309001856644337549"},"user_tz":180},"id":"7B1sTy5z2lLv"},"outputs":[],"source":["stops= set(stopwords.words('spanish'))\n","def preprocesamiento(documento):\n","# Eliminación de valores numéricos\n","  documento = re.sub('\\d', ' ', documento)\n","  documento = documento.lower()\n","############################## TOKENIZAR ##############################\n","# Tokenizo con una List Comprehension, y luego a esta le filtro las stopwords\n","  tokenizado = [token for token in regexp_tokenize(documento, pattern='\\w+')]\n","  filtro = [filtro for filtro in tokenizado if filtro not in stops]\n","############################## LEMATIZAR ##############################\n","# Spacy tiene su propio tokenizador \n","  doc= nlp(documento)\n","# Lematizo al texto, para eliminár los signos de puntuación o demas signos que no me aportarán al análisis, se filtrarán los tokens\n","# que están en la anterior List Comprehension llamada filtro\n","  lemma = [lema.lemma_ for lema in doc if str(lema) in filtro]\n","  Procesada=\" \".join(map(str, lemma))\n","  return Procesada"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["0       voz extremista conocido cuenta asociado movimi...\n","1       después casi dos año medio lucha política judi...\n","2       dos periodista birmano agencia reuters liberar...\n","3       cuerpo nacional policía detener cuatro persona...\n","4       desfile firma roma convertir oda libertad muje...\n","                              ...                        \n","1995    consejo gobierno dar visto bueno convocatoria ...\n","1996    investigador valenciano desarrollar innovador ...\n","1997    arrestado actuar coche establecimiento policía...\n","1998    rey encargar miércoles pedro sánchez presentar...\n","1999    prueba realizado centro nacional microbiología...\n","Name: Text, Length: 2000, dtype: object"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["pre_procesamiento=df['Text'].apply(preprocesamiento)\n","pre_procesamiento"]},{"cell_type":"markdown","metadata":{"id":"O8l0g9wa7gzm"},"source":["#### palabras mas frecuentes"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["'\\ndef ExtraeFeatures(documentos):\\n    features_aux = []\\n    features = []\\n    for documento in documentos:\\n        for feature in documento:\\n            features_aux.append(feature)\\n    for key in Counter(features_aux):\\n        features.append(key)\\n    return features\\n\\npalabras = ExtraeFeatures(pre_procesamiento)\\nlen(palabras)\\n'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","def ExtraeFeatures(documentos):\n","    features_aux = []\n","    features = []\n","    for documento in documentos:\n","        for feature in documento:\n","            features_aux.append(feature)\n","    for key in Counter(features_aux):\n","        features.append(key)\n","    return features\n","\n","palabras = ExtraeFeatures(pre_procesamiento)\n","len(palabras)\n","'''"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":802,"status":"ok","timestamp":1667178591238,"user":{"displayName":"Lautaro Santos Da Silveira","userId":"13309001856644337549"},"user_tz":180},"id":"0-nln3yrxmXG"},"outputs":[{"data":{"text/plain":["\"\\nfrom itertools import chain\\nlista= list(chain( * pre_procesamiento ))\\nx=Counter(lista).most_common()\\ndict_1=dict()\\nfor key,values in x: \\n    dict_1.setdefault(key, values)\\ndict_1 \\ndf1 = pd.DataFrame([[key, dict_1[key]] for key in dict_1.keys()], columns=['Name', 'Cant'])\\ngrupo=df.groupby(['Cant'])['Cant'].count().reset_index(name='cantidad')\\nlen(lista)\\n\""]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","from itertools import chain\n","lista= list(chain( * pre_procesamiento ))\n","x=Counter(lista).most_common()\n","dict_1=dict()\n","for key,values in x: \n","    dict_1.setdefault(key, values)\n","dict_1 \n","df1 = pd.DataFrame([[key, dict_1[key]] for key in dict_1.keys()], columns=['Name', 'Cant'])\n","grupo=df.groupby(['Cant'])['Cant'].count().reset_index(name='cantidad')\n","len(lista)\n","'''"]},{"cell_type":"markdown","metadata":{"id":"4ZoA5le37lpt"},"source":["#### continua"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abajo</th>\n","      <th>abalanzar</th>\n","      <th>abanderar</th>\n","      <th>abandonado</th>\n","      <th>abandonar</th>\n","      <th>abandono</th>\n","      <th>abascal</th>\n","      <th>abasccal</th>\n","      <th>abateír</th>\n","      <th>abatir</th>\n","      <th>...</th>\n","      <th>óscar</th>\n","      <th>últi</th>\n","      <th>últimamente</th>\n","      <th>último</th>\n","      <th>único</th>\n","      <th>útil</th>\n","      <th>рог</th>\n","      <th>сото</th>\n","      <th>тара</th>\n","      <th>те</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1995</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1996</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1997</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1998</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1999</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2000 rows × 9598 columns</p>\n","</div>"],"text/plain":["      abajo  abalanzar  abanderar  abandonado  abandonar  abandono  abascal  \\\n","0       0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","1       0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","2       0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","3       0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","4       0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","...     ...        ...        ...         ...        ...       ...      ...   \n","1995    0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","1996    0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","1997    0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","1998    0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","1999    0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","\n","      abasccal  abateír  abatir  ...  óscar  últi  últimamente  último  único  \\\n","0          0.0      0.0     0.0  ...    0.0   0.0          0.0     0.0    0.0   \n","1          0.0      0.0     0.0  ...    0.0   0.0          0.0     0.0    0.0   \n","2          0.0      0.0     0.0  ...    0.0   0.0          0.0     0.0    0.0   \n","3          0.0      0.0     0.0  ...    0.0   0.0          0.0     0.0    0.0   \n","4          0.0      0.0     0.0  ...    0.0   0.0          0.0     0.0    0.0   \n","...        ...      ...     ...  ...    ...   ...          ...     ...    ...   \n","1995       0.0      0.0     0.0  ...    0.0   0.0          0.0     0.0    0.0   \n","1996       0.0      0.0     0.0  ...    0.0   0.0          0.0     0.0    0.0   \n","1997       0.0      0.0     0.0  ...    0.0   0.0          0.0     0.0    0.0   \n","1998       0.0      0.0     0.0  ...    0.0   0.0          0.0     0.0    0.0   \n","1999       0.0      0.0     0.0  ...    0.0   0.0          0.0     0.0    0.0   \n","\n","      útil  рог  сото  тара   те  \n","0      0.0  0.0   0.0   0.0  0.0  \n","1      0.0  0.0   0.0   0.0  0.0  \n","2      0.0  0.0   0.0   0.0  0.0  \n","3      0.0  0.0   0.0   0.0  0.0  \n","4      0.0  0.0   0.0   0.0  0.0  \n","...    ...  ...   ...   ...  ...  \n","1995   0.0  0.0   0.0   0.0  0.0  \n","1996   0.0  0.0   0.0   0.0  0.0  \n","1997   0.0  0.0   0.0   0.0  0.0  \n","1998   0.0  0.0   0.0   0.0  0.0  \n","1999   0.0  0.0   0.0   0.0  0.0  \n","\n","[2000 rows x 9598 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["vectorizer = TfidfVectorizer()\n","vectores = vectorizer.fit_transform(pre_procesamiento).toarray()\n","nombres= vectorizer.get_feature_names_out()\n","df_vector= pd.DataFrame(vectores,columns=nombres )\n","df_vector\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abajo</th>\n","      <th>abalanzar</th>\n","      <th>abanderar</th>\n","      <th>abandonado</th>\n","      <th>abandonar</th>\n","      <th>abandono</th>\n","      <th>abascal</th>\n","      <th>abasccal</th>\n","      <th>abateír</th>\n","      <th>abatir</th>\n","      <th>...</th>\n","      <th>últi</th>\n","      <th>últimamente</th>\n","      <th>último</th>\n","      <th>único</th>\n","      <th>útil</th>\n","      <th>рог</th>\n","      <th>сото</th>\n","      <th>тара</th>\n","      <th>те</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1995</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1996</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1997</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1998</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1999</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2000 rows × 9599 columns</p>\n","</div>"],"text/plain":["      abajo  abalanzar  abanderar  abandonado  abandonar  abandono  abascal  \\\n","0       0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","1       0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","2       0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","3       0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","4       0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","...     ...        ...        ...         ...        ...       ...      ...   \n","1995    0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","1996    0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","1997    0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","1998    0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","1999    0.0        0.0        0.0         0.0        0.0       0.0      0.0   \n","\n","      abasccal  abateír  abatir  ...  últi  últimamente  último  único  útil  \\\n","0          0.0      0.0     0.0  ...   0.0          0.0     0.0    0.0   0.0   \n","1          0.0      0.0     0.0  ...   0.0          0.0     0.0    0.0   0.0   \n","2          0.0      0.0     0.0  ...   0.0          0.0     0.0    0.0   0.0   \n","3          0.0      0.0     0.0  ...   0.0          0.0     0.0    0.0   0.0   \n","4          0.0      0.0     0.0  ...   0.0          0.0     0.0    0.0   0.0   \n","...        ...      ...     ...  ...   ...          ...     ...    ...   ...   \n","1995       0.0      0.0     0.0  ...   0.0          0.0     0.0    0.0   0.0   \n","1996       0.0      0.0     0.0  ...   0.0          0.0     0.0    0.0   0.0   \n","1997       0.0      0.0     0.0  ...   0.0          0.0     0.0    0.0   0.0   \n","1998       0.0      0.0     0.0  ...   0.0          0.0     0.0    0.0   0.0   \n","1999       0.0      0.0     0.0  ...   0.0          0.0     0.0    0.0   0.0   \n","\n","      рог  сото  тара   те  class  \n","0     0.0   0.0   0.0  0.0   True  \n","1     0.0   0.0   0.0  0.0   True  \n","2     0.0   0.0   0.0  0.0   True  \n","3     0.0   0.0   0.0  0.0   True  \n","4     0.0   0.0   0.0  0.0   True  \n","...   ...   ...   ...  ...    ...  \n","1995  0.0   0.0   0.0  0.0   True  \n","1996  0.0   0.0   0.0  0.0   True  \n","1997  0.0   0.0   0.0  0.0   True  \n","1998  0.0   0.0   0.0  0.0   True  \n","1999  0.0   0.0   0.0  0.0   True  \n","\n","[2000 rows x 9599 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df_vector['class']=df['class']\n","df_vector"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["df_vector=shuffle(df_vector, random_state=123)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["x= df_vector.drop('class',axis=1)\n","y= df_vector['class']"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.2,random_state=123)"]},{"cell_type":"markdown","metadata":{},"source":["### Modelos"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, cross_validate\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Creamos los folds externos\n","skfold_outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n","\n","# Creamos los folds internos\n","skfold_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)"]},{"cell_type":"markdown","metadata":{},"source":["#### Arbol de Decision"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fit_time</th>\n","      <th>score_time</th>\n","      <th>test_score</th>\n","      <th>train_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>345.853230</td>\n","      <td>0.264001</td>\n","      <td>0.6425</td>\n","      <td>0.866875</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>470.574492</td>\n","      <td>0.160007</td>\n","      <td>0.6475</td>\n","      <td>0.849375</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>303.671870</td>\n","      <td>0.229440</td>\n","      <td>0.6575</td>\n","      <td>0.800625</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>339.033980</td>\n","      <td>0.135881</td>\n","      <td>0.6125</td>\n","      <td>0.818125</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>322.714527</td>\n","      <td>0.197015</td>\n","      <td>0.6125</td>\n","      <td>0.804375</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     fit_time  score_time  test_score  train_score\n","0  345.853230    0.264001      0.6425     0.866875\n","1  470.574492    0.160007      0.6475     0.849375\n","2  303.671870    0.229440      0.6575     0.800625\n","3  339.033980    0.135881      0.6125     0.818125\n","4  322.714527    0.197015      0.6125     0.804375"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Instanciamos el estimador\n","arbol= DecisionTreeClassifier(random_state=123)\n","\n","# Creamos la grilla de hiperparametros a probar\n","param_grid_arbol = [{'max_depth':[35,40,45], 'criterion':['gini','entropy']}]\n","\n","# Creamos el inner CV (GSCV para buscar los mejores hiperparámetros con los k folds internos)\n","gscv_arbol = GridSearchCV(estimator=arbol,\n","                    param_grid=param_grid_arbol, \n","                    scoring='accuracy',\n","                    n_jobs=-1,\n","                    cv=skfold_inner,\n","                    verbose=0,\n","                    refit=True)\n","\n","# Ahora creamos el outer CV para evaluar el inner CV para cada fold externo\n","# Recordar que cross_val_score/cross_validate realiza el .fit y .predict internamente\n","# Tener en cuenta que el .predict lo realiza sobre el modelo que tuvo mejores resultados en el entrenamiento (gscv_knn.best_estimator_)\n","nested_cv_arbol = cross_validate(gscv_arbol, X=x, y=y, cv=skfold_outer, return_train_score=True)\n","\n","result_cv_arbol = pd.DataFrame(nested_cv_arbol)\n","result_cv_arbol"]},{"cell_type":"markdown","metadata":{},"source":["#### SVM"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fit_time</th>\n","      <th>score_time</th>\n","      <th>test_score</th>\n","      <th>train_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>293.568247</td>\n","      <td>7.035432</td>\n","      <td>0.7600</td>\n","      <td>0.999375</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>299.334985</td>\n","      <td>7.187690</td>\n","      <td>0.7825</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>311.202961</td>\n","      <td>8.936035</td>\n","      <td>0.7575</td>\n","      <td>0.999375</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>270.767579</td>\n","      <td>8.928568</td>\n","      <td>0.8150</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>269.762343</td>\n","      <td>7.573648</td>\n","      <td>0.8275</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     fit_time  score_time  test_score  train_score\n","0  293.568247    7.035432      0.7600     0.999375\n","1  299.334985    7.187690      0.7825     1.000000\n","2  311.202961    8.936035      0.7575     0.999375\n","3  270.767579    8.928568      0.8150     1.000000\n","4  269.762343    7.573648      0.8275     1.000000"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Instanciamos el estimador\n","svc= SVC(random_state=123)\n","\n","# Creamos la grilla de hiperparametros a probar\n","param_grid_svc = [{'C':[0.0001,0.01,0.1,1], 'gamma':['scale','auto']}]\n","\n","# Creamos el inner CV (GSCV para buscar los mejores hiperparámetros con los k folds internos)\n","gscv_svc = GridSearchCV(estimator=svc,\n","                    param_grid=param_grid_svc, \n","                    scoring='accuracy',\n","                    n_jobs=-1,\n","                    cv=skfold_inner,\n","                    verbose=0,\n","                    refit=True)\n","\n","# Ahora creamos el outer CV para evaluar el inner CV para cada fold externo\n","# Recordar que cross_val_score/cross_validate realiza el .fit y .predict internamente\n","# Tener en cuenta que el .predict lo realiza sobre el modelo que tuvo mejores resultados en el entrenamiento (gscv_knn.best_estimator_)\n","nested_cv_svc = cross_validate(gscv_svc, X=x, y=y, cv=skfold_outer, return_train_score=True)\n","\n","result_cv_svc = pd.DataFrame(nested_cv_svc)\n","result_cv_svc"]},{"cell_type":"markdown","metadata":{},"source":["#### Random Forest"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fit_time</th>\n","      <th>score_time</th>\n","      <th>test_score</th>\n","      <th>train_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>301.761676</td>\n","      <td>0.182981</td>\n","      <td>0.6600</td>\n","      <td>0.966875</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>279.123584</td>\n","      <td>0.137009</td>\n","      <td>0.7100</td>\n","      <td>0.964375</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>263.611159</td>\n","      <td>0.124996</td>\n","      <td>0.6600</td>\n","      <td>0.972500</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>261.795267</td>\n","      <td>0.099033</td>\n","      <td>0.7200</td>\n","      <td>0.953125</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>264.132137</td>\n","      <td>0.102997</td>\n","      <td>0.7025</td>\n","      <td>0.966875</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     fit_time  score_time  test_score  train_score\n","0  301.761676    0.182981      0.6600     0.966875\n","1  279.123584    0.137009      0.7100     0.964375\n","2  263.611159    0.124996      0.6600     0.972500\n","3  261.795267    0.099033      0.7200     0.953125\n","4  264.132137    0.102997      0.7025     0.966875"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# Instanciamos el estimador\n","random= RandomForestClassifier(random_state=123)\n","\n","# Creamos la grilla de hiperparametros a probar\n","param_grid_random = [{'n_estimators':[35,40,45], 'criterion':['gini','entropy'], 'max_depth':[35,40,45]}]\n","\n","# Creamos el inner CV (GSCV para buscar los mejores hiperparámetros con los k folds internos)\n","gscv_random = GridSearchCV(estimator=random,\n","                    param_grid=param_grid_random, \n","                    scoring='accuracy',\n","                    n_jobs=-1,\n","                    cv=skfold_inner,\n","                    verbose=0,\n","                    refit=True)\n","\n","# Ahora creamos el outer CV para evaluar el inner CV para cada fold externo\n","# Recordar que cross_val_score/cross_validate realiza el .fit y .predict internamente\n","# Tener en cuenta que el .predict lo realiza sobre el modelo que tuvo mejores resultados en el entrenamiento (gscv_knn.best_estimator_)\n","nested_cv_random = cross_validate(gscv_random, X=x, y=y, cv=skfold_outer, return_train_score=True)\n","\n","result_cv_random = pd.DataFrame(nested_cv_random)\n","result_cv_random"]},{"cell_type":"markdown","metadata":{},"source":["#### Regresion Logistica"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n","24 fits failed out of a total of 48.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","12 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1094, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n","    raise ValueError(\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","--------------------------------------------------------------------------------\n","12 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1094, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n","    raise ValueError(\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.58863803        nan 0.75750059        nan 0.7481127\n","        nan 0.75750059        nan 0.753117          nan 0.75750059\n","        nan 0.76187364        nan 0.75750059]\n","  warnings.warn(\n","c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n","24 fits failed out of a total of 48.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","12 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1094, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n","    raise ValueError(\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","--------------------------------------------------------------------------------\n","12 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1094, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n","    raise ValueError(\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.58489271        nan 0.7668838         nan 0.75625098\n","        nan 0.7668838         nan 0.76437989        nan 0.7668838\n","        nan 0.77375607        nan 0.7668838 ]\n","  warnings.warn(\n","c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n","24 fits failed out of a total of 48.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","12 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1094, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n","    raise ValueError(\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","--------------------------------------------------------------------------------\n","12 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1094, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n","    raise ValueError(\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.59113491        nan 0.76436935        nan 0.74372911\n","        nan 0.76436935        nan 0.75810842        nan 0.76436935\n","        nan 0.76811467        nan 0.76436935]\n","  warnings.warn(\n","c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n","24 fits failed out of a total of 48.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","12 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1094, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n","    raise ValueError(\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","--------------------------------------------------------------------------------\n","12 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1094, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n","    raise ValueError(\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.57240832        nan 0.75875606        nan 0.72875603\n","        nan 0.75875606        nan 0.74376542        nan 0.75875606\n","        nan 0.75876777        nan 0.75875606]\n","  warnings.warn(\n","c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n","24 fits failed out of a total of 48.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","12 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1094, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n","    raise ValueError(\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","--------------------------------------------------------------------------------\n","12 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1094, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n","    raise ValueError(\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","c:\\Users\\Usuario\\Desktop\\lautaro\\ies 21\\entorno\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.58052317        nan 0.73688143        nan 0.72624276\n","        nan 0.73688143        nan 0.73561894        nan 0.73688143\n","        nan 0.73936894        nan 0.73688143]\n","  warnings.warn(\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fit_time</th>\n","      <th>score_time</th>\n","      <th>test_score</th>\n","      <th>train_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>26.165942</td>\n","      <td>0.184003</td>\n","      <td>0.7350</td>\n","      <td>0.971250</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>16.744107</td>\n","      <td>0.091015</td>\n","      <td>0.7700</td>\n","      <td>0.975000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>16.015795</td>\n","      <td>0.104002</td>\n","      <td>0.7500</td>\n","      <td>0.976875</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>14.159440</td>\n","      <td>0.089303</td>\n","      <td>0.7800</td>\n","      <td>0.974375</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>14.456590</td>\n","      <td>0.120014</td>\n","      <td>0.8075</td>\n","      <td>0.975625</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    fit_time  score_time  test_score  train_score\n","0  26.165942    0.184003      0.7350     0.971250\n","1  16.744107    0.091015      0.7700     0.975000\n","2  16.015795    0.104002      0.7500     0.976875\n","3  14.159440    0.089303      0.7800     0.974375\n","4  14.456590    0.120014      0.8075     0.975625"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# Instanciamos el estimador\n","RL= LogisticRegression(random_state=123)\n","\n","# Creamos la grilla de hiperparametros a probar\n","param_grid_RL = [{'C':[0.0001,0.01,0.1,1], 'penalty':['l1','l2','elasticnet','none']}]\n","\n","# Creamos el inner CV (GSCV para buscar los mejores hiperparámetros con los k folds internos)\n","gscv_RL = GridSearchCV(estimator=RL,\n","                    param_grid=param_grid_RL, \n","                    scoring='accuracy',\n","                    n_jobs=-1,\n","                    cv=skfold_inner,\n","                    verbose=0,\n","                    refit=True)\n","\n","# Ahora creamos el outer CV para evaluar el inner CV para cada fold externo\n","# Recordar que cross_val_score/cross_validate realiza el .fit y .predict internamente\n","# Tener en cuenta que el .predict lo realiza sobre el modelo que tuvo mejores resultados en el entrenamiento (gscv_knn.best_estimator_)\n","nested_cv_RL = cross_validate(gscv_RL, X=x, y=y, cv=skfold_outer, return_train_score=True)\n","\n","result_cv_RL = pd.DataFrame(nested_cv_RL)\n","result_cv_RL"]},{"cell_type":"markdown","metadata":{},"source":["#### Comparacion de modelos"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["Arbol_promedio = result_cv_arbol.test_score.mean()\n","Arbol_desvio = result_cv_arbol.test_score.std()\n","\n","SVC_promedio = result_cv_svc.test_score.mean()\n","SVC_desvio = result_cv_svc.test_score.std()\n","\n","RandomForest_promedio = result_cv_random.test_score.mean()\n","RandomForest_desvio = result_cv_random.test_score.std()\n","\n","RL_promedio = result_cv_RL.test_score.mean()\n","RL_desvio = result_cv_RL.test_score.std()"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Arbol promedio en el outer:  0.6345000000000001  +/-  0.04159326868617076  (95%)\n","SVC promedio en el outer:  0.7885  +/-  0.0635019684734261  (95%)\n","Random Forest promedio en el outer:  0.6905  +/-  0.05705260730238359  (95%)\n","Regresion Logistica promedio en el outer:  0.7685000000000001  +/-  0.05585696017507578  (95%)\n"]}],"source":["print('Arbol promedio en el outer: ',Arbol_promedio, \" +/- \", 2*Arbol_desvio, \" (95%)\")\n","print('SVC promedio en el outer: ',SVC_promedio, \" +/- \", 2*SVC_desvio, \" (95%)\")\n","print('Random Forest promedio en el outer: ',RandomForest_promedio, \" +/- \", 2*RandomForest_desvio, \" (95%)\")\n","print('Regresion Logistica promedio en el outer: ',RL_promedio, \" +/- \", 2*RL_desvio, \" (95%)\")"]},{"cell_type":"markdown","metadata":{},"source":["El mejor modelo que me dio es el de SVC, ahora procedo a la busqueda de mejores hiperparametros"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["svc= SVC(random_state=123)\n","\n","# Creamos la grilla de hiperparametros a probar\n","param_grid_svc = [{'C':[0.0001,0.01,0.1,1], 'gamma':['scale','auto']}]\n","\n","# Creamos el inner CV (GSCV para buscar los mejores hiperparámetros con los k folds internos)\n","gscv_svc = GridSearchCV(estimator=svc,\n","                    param_grid=param_grid_svc, \n","                    scoring='accuracy',\n","                    n_jobs=-1,\n","                    cv=skfold_inner,\n","                    verbose=0,\n","                    refit=True)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["algoritmo_elegido = gscv_svc\n","algoritmo_elegido.fit(x, y)\n","\n","elegido_resultados = pd.DataFrame(algoritmo_elegido.cv_results_).loc[algoritmo_elegido.best_index_]\n","\n","elegido_promedio = elegido_resultados.mean_test_score\n","elegido_desvio = elegido_resultados.std_test_score\n","\n","print('AC: ', elegido_promedio*100, \" +/- \", 2*elegido_desvio*100, \" (95%)\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN29Bmrlmw3Cblt+MO5Bg2c","collapsed_sections":[],"mount_file_id":"1HFowf9SP7MZlKv6SglpgvBfOsA13pHzg","provenance":[]},"kernelspec":{"display_name":"Python 3.10.4 ('entorno': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"e8bee0e65890cdfebbb107e82a71c40ffc29519aefa63d0fd30a5a41d03c7b29"}}},"nbformat":4,"nbformat_minor":0}
